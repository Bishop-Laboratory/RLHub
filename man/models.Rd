% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/models.R
\name{fft_model}
\alias{fft_model}
\alias{models}
\alias{prep_features}
\title{Models}
\usage{
fft_model(quiet = FALSE)

prep_features(quiet = FALSE)
}
\arguments{
\item{quiet}{If TRUE, messages are suppressed. Default: FALSE.}
}
\value{
A model object from the \code{caret} package.
}
\description{
Models used for predicting sample label ("POS": robustly maps R-loops; "NEG": poorly maps R-loops )
based on R-loop-forming sequences analysis (\code{\link[RLSeq:analyzeRLFS]{RLSeq::analyzeRLFS()}}).
These models are used with \code{\link[RLSeq:predictCondition]{RLSeq::predictCondition()}}.
}
\details{
\subsection{Source}{

The models were developed as part of a semi-automated online learning scheme
found in the RLBase-data protocol \href{https://github.com/Bishop-Laboratory/RLBase-data#build-discriminator-model}{here}.
Briefly, R-loop-forming sequences (RLFS) analysis was performed using \code{\link[RLSeq:analyzeRLFS]{RLSeq::analyzeRLFS()}}
for every sample peakset in RLBase (see \link{rlfs_res} for full results). The
samples were then manually inspected and any which starkly differed from
their label were removed. Out of 693 possible samples, 135 were excluded
due to a mismatch with their label. The remaining steps were
performed automatically.
\itemize{
\item First,The non-discarded samples were partitioned 50:25:25 (train:test:discovery).
Feature transformation was performed on the full data-set using the "YeoJohnson"
transform along with typical standardization via \code{\link[caret:preProcess]{caret::preProcess()}}.
\item Then, feature selection was performed in the discovery set using \code{\link[Boruta:Boruta]{Boruta::Boruta()}}.
\item Then, the training set was then trained using a stacked ensemble model:
\itemize{
\item The ensemble model is a Random Forest and the 5 base models in the stack are:
\itemize{
\item Latent Dirichlet allocation
\item Recursive partitioning
\item Generalized linear model (logit)
\item K-nearest neighbors
\item Support vector machine (radial)
}
\item 10-fold 5-repeated cross-validation was implemented during training.
}
\item Finally, The model was then evaluated in the testing set. It demonstrates an
accuracy of 0.9043. For more details, see the HTML
report \href{https://rlbase-data.s3.amazonaws.com/misc/model/FFT-classifier.html}{here}.
}
}

\subsection{Structure}{
\itemize{
\item \code{prep_features()}
\itemize{
\item A feature-transform model which prepares the data for classification.
\item It is an object of class \code{preProcess} from the \code{\link[caret:preProcess]{caret::preProcess()}} function call.
}
\item \code{fft_model()}
\itemize{
\item A binary classifier which returns "POS" or "NEG".
\item It is an object of class \code{caretStack} from the \code{\link[caretEnsemble:caretList]{caretEnsemble::caretList()}} function call.
}
}
}

\subsection{Usage}{

These models are used internally by \code{\link[RLSeq:predictCondition]{RLSeq::predictCondition()}}.
}
}
\examples{

fftModel <- fft_model()

pfModel <- prep_features()
}
